<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/icon/favicon.png"><link rel="icon" href="/img/icon/favicon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content="练习写作，记录Life"><meta name="author" content="Emhui"><meta name="keywords" content=""><meta name="description" content="Spark初次上手：快速开始#@教程 #@spark 使用Spark Shell进行交互 软件下载地址  基本启动Spark，在刚刚下载的Spark包中解压，然后执行下面命令进行启动。  官方使用了Scala和Python两种语言都实现了一遍，如果是使用Python的话，则使用的是pyspark，且直接使用pip install pyspark安装即可使用。  1.&#x2F;bin&#x2F;spark-shell"><meta property="og:type" content="article"><meta property="og:title" content="Spark初次上手：快速开始"><meta property="og:url" content="http://emhui.fun/2021/05/26/Spark%E5%88%9D%E6%AC%A1%E4%B8%8A%E6%89%8B%EF%BC%9A%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/index.html"><meta property="og:site_name" content="Emhui的小站"><meta property="og:description" content="Spark初次上手：快速开始#@教程 #@spark 使用Spark Shell进行交互 软件下载地址  基本启动Spark，在刚刚下载的Spark包中解压，然后执行下面命令进行启动。  官方使用了Scala和Python两种语言都实现了一遍，如果是使用Python的话，则使用的是pyspark，且直接使用pip install pyspark安装即可使用。  1.&#x2F;bin&#x2F;spark-shell"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://gitee.com/yoyhm/oss/raw/master/uPic/Ehv0I5.png"><meta property="article:published_time" content="2021-05-26T03:18:32.000Z"><meta property="article:modified_time" content="2021-10-10T08:49:32.813Z"><meta property="article:author" content="Emhui"><meta property="article:tag" content="教程"><meta property="article:tag" content="spark"><meta property="article:tag" content="yarn"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://gitee.com/yoyhm/oss/raw/master/uPic/Ehv0I5.png"><title>Spark初次上手：快速开始 - Emhui的小站</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.4.0/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"emhui.fun",root:"/",version:"1.8.12",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,onlypost:!1},web_analytics:{enable:!0,baidu:"e2720b60fa1323d2ed667e95b6b213c7",google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null}},search_path:"/local-search.xml"}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.2.0"></head><body><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>浮生若梦H</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(https://api.dujin.org/bing/1366.php) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="Spark初次上手：快速开始"></span><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-05-26 11:18" pubdate>2021年5月26日 上午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 7.7k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> NaN 分钟</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">Spark初次上手：快速开始</h1><div class="markdown-body"><h1 id="Spark初次上手：快速开始"><a href="#Spark初次上手：快速开始" class="headerlink" title="Spark初次上手：快速开始"></a>Spark初次上手：快速开始</h1><p>#@教程 #@spark</p><h2 id="使用Spark-Shell进行交互"><a href="#使用Spark-Shell进行交互" class="headerlink" title="使用Spark Shell进行交互"></a>使用Spark Shell进行交互</h2><blockquote><p><a target="_blank" rel="noopener" href="https://spark.apache.org/downloads.html">软件下载地址</a></p></blockquote><h3 id="基本"><a href="#基本" class="headerlink" title="基本"></a>基本</h3><p>启动<code>Spark</code>，在刚刚下载的<code>Spark</code>包中解压，然后执行下面命令进行启动。</p><blockquote><p>官方使用了Scala和Python两种语言都实现了一遍，如果是使用Python的话，则使用的是<code>pyspark</code>，且直接使用<code>pip install pyspark</code>安装即可使用。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./bin/spark-shell<br></code></pre></td></tr></table></figure><p>读取文件操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">scala&gt;</span><span class="bash"> val textFile = spark.read.text(<span class="hljs-string">&quot;README.md&quot;</span>)</span><br>textFile: org.apache.spark.sql.DataFrame = [value: string]<br><span class="hljs-meta">scala&gt;</span><span class="bash"> textFile.count()</span><br>res0: Long = 108<br></code></pre></td></tr></table></figure><p>从上面可以看到，<code>Spark</code>读取文件，然后是以<strong>行</strong>作为单位的，因为<code>READM.md</code>文件中正好是108行。</p><p>读取第一行数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">scala&gt;</span><span class="bash"> textFile.first()</span><br>res1: org.apache.spark.sql.Row = [# Apache Spark]<br></code></pre></td></tr></table></figure><p>使用<code>filter</code>功能创建新的<code>Dataset</code>对象</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">scala&gt;</span><span class="bash"> val linesWithSpark = textFile.filter(line =&gt; line.toString().contains(<span class="hljs-string">&quot;Spark&quot;</span>))</span><br>linesWithSpark: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [value: string]<br><br><span class="hljs-meta">scala&gt;</span><span class="bash"> linesWithSpark.count()</span><br>res2: Long = 19<br></code></pre></td></tr></table></figure><blockquote><ul><li>⚠️： 官方这里使用的是<code>linesWithSpark = textFile.filter(line =&gt; line.contains(&quot;Spark&quot;))</code>，但是可能是版本不支持之类的。<code>line</code>没有<code>contains</code>方法。因此使用上面的写法才是正确的。</li><li>通过对<code>README.md</code>的统计，含有<code>Spark</code>的行的确是19行，因此上面的程序是✅。</li></ul></blockquote><p>使用<code>show</code>展示<code>Dataset</code>数据</p><blockquote><p>默认展示20条，可以传递参数到<code>show</code>中展示指定数量的数据。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">scala&gt;</span><span class="bash"> textFile.show()</span><br>+--------------------+<br>|               value|<br>+--------------------+<br>|      # Apache Spark|<br>|                    |<br>|Spark is a unifie...|<br>|high-level APIs i...|<br>|supports general ...|<br>|rich set of highe...|<br>|MLlib for machine...|<br>|and Structured St...|<br>|                    |<br>|&lt;https://spark.ap...|<br>|                    |<br>|[![Jenkins Build]...|<br>|[![AppVeyor Build...|<br>|[![PySpark Covera...|<br>|                    |<br>|                    |<br>|## Online Documen...|<br>|                    |<br>|You can find the ...|<br>|guide, on the [pr...|<br>+--------------------+<br>only showing top 20 rows<br></code></pre></td></tr></table></figure><h3 id="更多Dataset操作"><a href="#更多Dataset操作" class="headerlink" title="更多Dataset操作"></a>更多Dataset操作</h3><ol><li>需求：找到有最多单词的那行。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">scala&gt;</span><span class="bash"> textFile.map(line =&gt; line.toString().split(<span class="hljs-string">&quot; &quot;</span>).size).reduce((a, b) =&gt; <span class="hljs-keyword">if</span> (a &gt; b) a <span class="hljs-keyword">else</span> b)</span><br>res3: Int = 16<br></code></pre></td></tr></table></figure><p>解释：</p><ul><li>首先使用<code>map(line =&gt; line.toString().split(&quot; &quot;).size)</code>操作，统计每行的单词数量。</li><li><code>reduce</code>调用<code>Dataset</code>方法来统计单词数量最多的那行。</li></ul><p>使用下面的方法引入函数也能达到相同的效果。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">scala&gt; import java.lang.Math<br>import java.lang.Math<br><br>scala&gt; textFile.map(line =&gt; line.toString().split(<span class="hljs-string">&quot; &quot;</span>).size).reduce((a, b) =&gt; Math.max(a, b))<br>res22: Int = 16<br></code></pre></td></tr></table></figure><ol start="2"><li>实现<code>mapreduce</code>的流程。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">scala&gt;</span><span class="bash"> val wordCounts = textFile.flatMap(line =&gt; line.split(<span class="hljs-string">&quot; &quot;</span>)).groupByKey(identity).count()</span><br>wordCounts: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, count(1): bigint]<br></code></pre></td></tr></table></figure><p>解释：</p><ul><li><code>flatMap</code>将行数据转化为单词数据。</li><li><code>groupByKey</code>和<code>Count</code>将数据中的单词转化为&lt;String,Long&gt;对。</li><li><code>collect</code>用于在shell中收集这些单词。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">scala&gt;</span><span class="bash"> val wordCounts = textFile.flatMap(line =&gt; line.toString().split(<span class="hljs-string">&quot; &quot;</span>)).groupByKey(identity).count()</span><br>wordCounts: org.apache.spark.sql.Dataset[(String, Long)] = [key: string, count(1): bigint]<br><br><span class="hljs-meta">scala&gt;</span><span class="bash"> wordCounts.collect</span><br>collect   collectAsList<br><br><span class="hljs-meta">scala&gt;</span><span class="bash"> wordCounts.collect()</span><br>res24: Array[(String, Long)] = Array((given.],1), ([[run,1), (online,1), (graphs,1), (URL,],1), ([Spark,4), (Shell],2), (including],1), (documentation,3), (command,,2), (abbreviated,1), (using:],1), ([Please,4), (Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3)],1), (overview,1), (SparkPi],2), (set,2), (-DskipTests,1), (name,1), (stream,1), (not,1), (programs,2), (tests,1), (particular,2), ([for,2), ([in,1), (must,1), (using,3), (./build/mvn,1), (instructions.],1), (you,4), (Programs],1), (variable,1), (Note,1), (core,1), ([storage,1), ([#,1), (protocols,1), ([To,2), (guidance,2), (page](https://spark.apache.org/documenta...<br></code></pre></td></tr></table></figure><h2 id="Caching-缓存"><a href="#Caching-缓存" class="headerlink" title="Caching 缓存"></a>Caching 缓存</h2><p>支持将数据放到内存缓存中。</p><p> </p><h2 id="Self-Contained-Applications-独立应用的程序"><a href="#Self-Contained-Applications-独立应用的程序" class="headerlink" title="Self-Contained Applications 独立应用的程序"></a>Self-Contained Applications 独立应用的程序</h2><p>在Java中使用Spark API进行开发。</p><p>需求：编写一个统计包含<code>a</code>和包含<code>b</code>的行数数量。</p><ol><li>添加依赖</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependencies</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span> <span class="hljs-comment">&lt;!-- Spark dependency --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.spark<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spark-sql_2.12<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>3.1.1<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">scope</span>&gt;</span>provided<span class="hljs-tag">&lt;/<span class="hljs-name">scope</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependencies</span>&gt;</span><br></code></pre></td></tr></table></figure><ol start="2"><li>编写Java程序</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">import</span> org.apache.spark.api.java.function.FilterFunction;<br><span class="hljs-keyword">import</span> org.apache.spark.sql.Dataset;<br><span class="hljs-keyword">import</span> org.apache.spark.sql.SparkSession;<br><br><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SimpleApp</span> </span>&#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> String logFile = <span class="hljs-string">&quot;/usr/local/spark-3.1.1-bin-hadoop2.7/README.md&quot;</span>;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;<br>        <span class="hljs-comment">// SparkSession Spark程序的入口</span><br>        SparkSession sparkSession = SparkSession.builder().appName(<span class="hljs-string">&quot;Simple App&quot;</span>).getOrCreate();<br><br>        Dataset&lt;String&gt; dataset = sparkSession.read().textFile(logFile).cache();<br><br>        <span class="hljs-keyword">long</span> numAs = dataset.filter((FilterFunction&lt;String&gt;) s -&gt; s.contains(<span class="hljs-string">&quot;a&quot;</span>)).count();<br>        <span class="hljs-keyword">long</span> numBs = dataset.filter((FilterFunction&lt;String&gt;) s -&gt; s.contains(<span class="hljs-string">&quot;b&quot;</span>)).count();<br><br>        System.out.println(<span class="hljs-string">&quot;含有a的行数为：&quot;</span> + numAs);<br>        System.out.println(<span class="hljs-string">&quot;含有b的行数为：&quot;</span> + numBs);<br><br>        sparkSession.stop();<br><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><ol start="3"><li>打包成Jar包</li></ol><p>在项目的根目录下运行下面命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">mvn package<br></code></pre></td></tr></table></figure><ol start="4"><li>提交程序并且运行</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/usr/<span class="hljs-built_in">local</span>/spark-3.1.1-bin-hadoop2.7/bin/spark-submit --class <span class="hljs-string">&quot;SimpleApp&quot;</span> --master <span class="hljs-built_in">local</span> target/spark-learn-01-1.0-SNAPSHOT.jar<br></code></pre></td></tr></table></figure><ol start="5"><li>运行结果查看</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">......<br>21/05/26 10:35:01 INFO DAGScheduler: Job 1 finished: count at SimpleApp.java:15, took 0.125818 s<br>含有a的行数为：64<br>含有b的行数为：32<br>21/05/26 10:35:01 INFO SparkUI: Stopped Spark web UI at http://emhuidembp.lan:4040<br>.......<br></code></pre></td></tr></table></figure><h2 id="其他学习"><a href="#其他学习" class="headerlink" title="其他学习"></a>其他学习</h2><ul><li><p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">RDD Programing Guide</a></p></li><li><p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL Programing Guide</a></p></li><li><p>Spark 部署到集群中 <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/cluster-overview.html">deployment overview</a>.</p></li><li><p>Spark 自带案例运行</p></li></ul><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># For Scala and Java, use run-example:</span><br>.<span class="hljs-regexp">/bin/</span>run-example SparkPi<br><br><span class="hljs-comment"># For Python examples, use spark-submit directly:</span><br>.<span class="hljs-regexp">/bin/</span>spark-submit examples<span class="hljs-regexp">/src/m</span>ain<span class="hljs-regexp">/python/</span>pi.py<br><br><span class="hljs-comment"># For R examples, use spark-submit directly:</span><br>.<span class="hljs-regexp">/bin/</span>spark-submit examples<span class="hljs-regexp">/src/m</span>ain<span class="hljs-regexp">/r/</span>dataframe.R<br></code></pre></td></tr></table></figure><h2 id="【强化】部署到集群中运行"><a href="#【强化】部署到集群中运行" class="headerlink" title="【强化】部署到集群中运行"></a>【强化】部署到集群中运行</h2><p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/submitting-applications.html">根据官方文档</a>，把它部署到集群中去</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Run on a YARN cluster</span><br><span class="hljs-built_in">export</span> HADOOP_CONF_DIR=XXX<br>./bin/spark-submit \<br>  --class org.apache.spark.examples.SparkPi \<br>  --master yarn \<br>  --deploy-mode cluster \  <span class="hljs-comment"># can be client for client mode</span><br>  --executor-memory 20G \<br>  --num-executors 50 \<br>  /path/to/examples.jar \<br>  1000<br></code></pre></td></tr></table></figure><p>根据上面进行改写</p><blockquote><p>⚠️：提交任务前需要先使用 <code>export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/</code>配置环境。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> HADOOP_CONF_DIR=/usr/<span class="hljs-built_in">local</span>/hadoop/etc/hadoop/<br><span class="hljs-built_in">echo</span> <span class="hljs-variable">$HADOOP_CONF_DIR</span> <span class="hljs-comment"># 验证是否成功</span><br><span class="hljs-comment"># 运行命令</span><br>/usr/<span class="hljs-built_in">local</span>/spark-3.1.1-bin-hadoop2.7/bin/spark-submit --class <span class="hljs-string">&quot;SimpleApp&quot;</span> --master yarn target/spark-learn-01-1.0-SNAPSHOT.jar<br></code></pre></td></tr></table></figure><p>这里会报下面错误</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">Exception <span class="hljs-keyword">in</span> thread <span class="hljs-string">&quot;main&quot;</span> org.apache.spark.sql.AnalysisException: Path does not exist: hdfs://localhost:9000/usr/<span class="hljs-built_in">local</span>/spark-3.1.1-bin-hadoop2.7/README.md<br></code></pre></td></tr></table></figure><p>因此需要对程序进行改写</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SimpleApp</span> </span>&#123;<br>    <span class="hljs-comment">// private static final String logFile = &quot;/usr/local/spark-3.1.1-bin-hadoop2.7/README.md&quot;;</span><br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;<br>        <span class="hljs-comment">// SparkSession Spark程序的入口</span><br>        SparkSession sparkSession = SparkSession.builder().appName(<span class="hljs-string">&quot;Simple App&quot;</span>).getOrCreate();<br>      	<span class="hljs-comment">// 修改为从参数中读取内容</span><br>        String logFile = args[<span class="hljs-number">0</span>];<br><br>        <span class="hljs-comment">// 读取数据</span><br>        Dataset&lt;String&gt; dataset = sparkSession.read().textFile(logFile).cache();<br><br>        <span class="hljs-keyword">long</span> numAs = dataset.filter((FilterFunction&lt;String&gt;) s -&gt; s.contains(<span class="hljs-string">&quot;a&quot;</span>)).count();<br>        <span class="hljs-keyword">long</span> numBs = dataset.filter((FilterFunction&lt;String&gt;) s -&gt; s.contains(<span class="hljs-string">&quot;b&quot;</span>)).count();<br><br>        System.out.println(<span class="hljs-string">&quot;含有a的行数为：&quot;</span> + numAs);<br>        System.out.println(<span class="hljs-string">&quot;含有b的行数为：&quot;</span> + numBs);<br><br>        sparkSession.stop();<br><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>这里将运行命令也进行改写，同时需要指定输入参数。先把<code>README.md</code>文件放到HDFS中去。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">(base) ➜  code hdfs dfs -mkdir spark<br>2021-05-26 11:11:34,856 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="hljs-keyword">for</span> your platform... using builtin-java classes <span class="hljs-built_in">where</span> applicable<br>(base) ➜  code hdfs dfs -put /usr/<span class="hljs-built_in">local</span>/spark-3.1.1-bin-hadoop2.7/README.md spark<br>2021-05-26 11:11:54,876 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="hljs-keyword">for</span> your platform... using builtin-java classes <span class="hljs-built_in">where</span> applicable<br>(base) ➜  code hdfs dfs -ls spark<br>2021-05-26 11:12:08,403 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="hljs-keyword">for</span> your platform... using builtin-java classes <span class="hljs-built_in">where</span> applicable<br>Found 1 items<br>-rw-r--r--   1 emhui supergroup       4488 2021-05-26 11:11 spark/README.md<br></code></pre></td></tr></table></figure><p>将命令写成脚本<code>run.sh</code>运行:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">(base) ➜  code cat script/run.sh<br>export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/<br>/usr/local/spark-3.1.1-bin-hadoop2.7/bin/spark-submit --class SimpleApp --master yarn target/spark-learn-01-1.0-SNAPSHOT.jar spark/README.md<br></code></pre></td></tr></table></figure><p>运行该命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sh script/run.sh<br></code></pre></td></tr></table></figure><p>可以看到结果也正确运行，且Yarn中也有该任务。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">2021-05-26 11:14:22,526 INFO scheduler.DAGScheduler: Job 1 finished: count at SimpleApp.java:16, took 1.124766 s<br>含有a的行数为：64<br>含有b的行数为：32<br>2021-05-26 11:14:22,549 INFO server.AbstractConnector: Stopped Spark@55b5e331&#123;HTTP/1.1, (http/1.1)&#125;&#123;0.0.0.0:4040&#125;<br></code></pre></td></tr></table></figure><p><img src="https://gitee.com/yoyhm/oss/raw/master/uPic/Ehv0I5.png" srcset="/img/loading.gif" lazyload alt="Ehv0I5"></p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/%E6%95%99%E7%A8%8B/">教程</a> <a class="hover-with-bg" href="/tags/spark/">spark</a> <a class="hover-with-bg" href="/tags/yarn/">yarn</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/10/10/M1%E8%8A%AF%E7%89%87MacBook%E7%9A%84VSCODE%E4%BD%BF%E7%94%A8C-%E8%B0%83%E8%AF%95%E4%BB%A3%E7%A0%81/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">M1芯片MacBook的VSCODE使用C++调试代码</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/04/17/JdbcTemplate%E7%9A%84%E4%BD%BF%E7%94%A8/"><span class="hidden-mobile">JdbcTemplate的使用</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><script type="text/javascript">Fluid.utils.loadComments("#comments",(function(){var t=document.documentElement.getAttribute("data-user-color-scheme");t="dark"===t?"github-dark":"github-light",window.UtterancesThemeLight="github-light",window.UtterancesThemeDark="github-dark";var e=document.createElement("script");e.setAttribute("src","https://utteranc.es/client.js"),e.setAttribute("repo","emhui/comments"),e.setAttribute("issue-term","pathname"),e.setAttribute("label","utterances"),e.setAttribute("theme",t),e.setAttribute("crossorigin","anonymous"),document.getElementById("comments").appendChild(e)}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="busuanzi_container_site_pv" style="display:none">总访问量 <span id="busuanzi_value_site_pv"></span> 次 </span><span id="busuanzi_container_site_uv" style="display:none">总访客数 <span id="busuanzi_value_site_uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/local-search.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.0/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script><script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script><script>!function(t,i){(0,Fluid.plugins.typing)(i.getElementById("subtitle").title)}(window,document)</script><script defer>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e2720b60fa1323d2ed667e95b6b213c7";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script src="/js/boot.js"></script></body></html>